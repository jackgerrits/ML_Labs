{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will learn how to do classification in Python using scikit and also see how to do the same using Vowpal Wabbit. We will classify a piece of text into two categories - spam and not spam (called ham). We will also learn about feature representation using Bag of Words (BOW)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1) Data exploration: Load and understand data**\n",
    "\n",
    "**(2) Feature engineering**\n",
    "     - Feature Extraction/Engineering using Bag Of Words (BOW)\n",
    "     - Data preparation\n",
    "     - Tranform data into BOW\n",
    "\n",
    "**(3) Classification Metrics**\n",
    "\n",
    "**(4) Classifiers and model evaluation**\n",
    "        - Majority Class classifier\n",
    "        - Logistic regression for classification\n",
    "        - Classification using Vowpal Wabbit\n",
    "\n",
    "**(5) Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration: Load and understand data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg \n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data into a dataframe using pandas\n",
    "df = pd.read_csv('spam_ham_v23.csv', sep = \",\")\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the first few samples\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the last few samples (notice the file seems to be sorted, so it's important that we shuffle it later)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Category').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the package we will be using handles only numerical values for labels. So, let's map our classes to reals.\n",
    "def return_label(x):\n",
    "    if x==\"ham\":\n",
    "        return 0.\n",
    "    else:\n",
    "        return 1.\n",
    "    \n",
    "df['label'] = df['Category'].apply(return_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that we have must be conducive to learning. This is where one would need some domain knowledge. Just consider how you would try to yourself classify text into spam or not spam in a language you have absolutely no familiarity with. \n",
    "\n",
    "\n",
    "You don't have enough time to actually learn the language, so you want to find heuristics that will help you be an accurate classifier. This will give you a clue as to what might be useful to a computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at message length\n",
    "df['msg_len'] = df['Message'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intution for splitting data into train and test:\n",
    "\n",
    "When you want to measure how well you have learnt something, you want to do some practice problems or take a practice exam. That is exactly what is happening here. You are putting some questions away as practice questions for yourself and learning from the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, testing_data = train_test_split(df,random_state = 2019, test_size = 0.2)\n",
    "\n",
    "Y_train=training_data['label'].values\n",
    "Y_test=testing_data['label'].values\n",
    "\n",
    "testing_data.groupby('label')['label'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[training_data['Category'] == 'ham']['msg_len'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter the dataframe by category (i.e. spam or ham) and then generate histograms that show how many messages are there for a particular message length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_ham = training_data[training_data['Category'] == 'ham']['msg_len']\n",
    "plt.figure(figsize = (10,5))\n",
    "plt.xlim(0,10000)\n",
    "plt.hist(hist_ham, bins=range(min(hist_ham), max(hist_ham) + 10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[training_data['Category'] == 'spam']['msg_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_spam = training_data[training_data['Category'] == 'spam']['msg_len']\n",
    "plt.figure(figsize = (10,5))\n",
    "plt.xlim(0,10000)\n",
    "plt.hist(hist_spam, bins=range(min(hist_ham), max(hist_ham) + 10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like message length could be a good indicator of the category of message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction/Engineering using Bag Of Words (BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with text, one needs to be careful. Right now, we are not linguists who understand the semantics of the language. We want to deal with this abstractly. We want our method to be language agnostic. \n",
    "\n",
    "BOW is one such method. A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
    "\n",
    " - A vocabulary of known words.\n",
    " - A measure of the presence of known words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"Pavithra likes everything Harry Potter \"\n",
    "s2 = \"Pavithra is a Potter Head \"\n",
    "s3 = \"Clearly, Pavithra likes talking about pavithra \"\n",
    "\n",
    "# Tokenization\n",
    "print(s1.split())\n",
    "print(s2.split())\n",
    "print(s3.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = count_vec.fit_transform([s3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec.get_feature_names()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [s1,s2,s3]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = mpimg.imread(\"BOW.PNG\")\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = count_vec.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.todense())\n",
    "print(count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = count_vec.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.todense())\n",
    "print(count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = \"Pavithra also likes LOTR\"\n",
    "doc_mat = count_vec.transform([s1,s2,s3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc_mat.todense())\n",
    "print(count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Notice difference between fit_transform and transform\n",
    " - fit_transform: Learn the vocabulary dictionary and return term-document matrix.\n",
    " - transform: Transform documents to document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_mat.nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sparsity: %.2f%%\" % (100.0 * doc_mat.nnz/ (doc_mat.shape[0] * doc_mat.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform data into BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_transformer = CountVectorizer(stop_words='english')\n",
    "BOW_transformer.fit_transform(training_data['Message'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_BOW_features = BOW_transformer.transform(training_data['Message'].values)\n",
    "test_BOW_features = BOW_transformer.transform(testing_data['Message'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append length of message to features\n",
    "from scipy.sparse import hstack\n",
    "X_train = hstack((train_BOW_features,training_data['msg_len'].values[:,None]))\n",
    "X_test = hstack((test_BOW_features,testing_data['msg_len'].values[:,None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our classifier make predictions, we want some notion of how good our classifier is. Below are three metrics that are most commonly used in classification.\n",
    "\n",
    "(1) Accuracy\n",
    "-  How many of your predictions matched the ground truth?\n",
    "\n",
    "(2) Precision\n",
    "-  Among the ones that you predicted as spam, how many were actually spam?\n",
    "\n",
    "(3) Recall\n",
    "- Among the ones that were actually spam, how many did you predict as spam?\n",
    "\n",
    "\n",
    "**Which ones should we care about?**\n",
    "\n",
    "It depends on the problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How well does a majority class classifier do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) What is a majority classifier?\n",
    "\n",
    "- A majority classifier is one that predicts the majority class in the dataset for every sample. \n",
    "\n",
    "(2) Why should we care about this?\n",
    "\n",
    "- This gives us a **baseline** to compare to. This is the simplest thing you can do if you had no access to anything else but the labels. One would expect our algorithms to do better than this.\n",
    "\n",
    "Caveat: the notion of majority class is valid only in binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_majority = np.array([0.]*len(testing_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(truth, prediction):\n",
    "    count_correct_pred = 0\n",
    "    for i in range(len(truth)):\n",
    "        if truth[i] == prediction[i]:\n",
    "            count_correct_pred +=1\n",
    "    return count_correct_pred/len(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy(Y_test, pred_majority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# alternative way to compute accuracy\n",
    "np.mean(Y_test == pred_majority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(truth, prediction):\n",
    "    count_prec = 0\n",
    "    for i in range(len(truth)):\n",
    "        if prediction[i] == 1. and truth[i] == 1.:\n",
    "            count_prec +=1\n",
    "    if np.sum(prediction) == 0.:\n",
    "        return 0.\n",
    "    else:\n",
    "        return count_prec/np.sum(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_precision(Y_test, pred_majority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative way to compute precision\n",
    "np.mean(Y_test[pred_majority == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall(truth, prediction):\n",
    "    count_recall = 0\n",
    "    for i in range(len(truth)):\n",
    "        if prediction[i] == 1. and truth[i] == 1.:\n",
    "            count_recall +=1\n",
    "    if np.sum(prediction) == 0.:\n",
    "        return 0.\n",
    "    else:\n",
    "        return count_recall/np.sum(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recall(Y_test, pred_majority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative way to compute recall\n",
    "np.mean(pred_majority[Y_test == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "print(metrics.accuracy_score(Y_test,pred_majority))\n",
    "\n",
    "print(metrics.precision_score(Y_test,pred_majority))\n",
    "\n",
    "print(metrics.recall_score(Y_test,pred_majority))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix: \")\n",
    "print(metrics.confusion_matrix(Y_test, pred_majority))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a statistical model that is used in classification. It can be used for binary and multi class classification. It uses the logistic function as the loss function. For more info: [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = mpimg.imread(\"LR.PNG\")\n",
    "plt.figure(figsize = (10,15))\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "Spam_model = LogisticRegression(solver='liblinear', penalty='l1') \n",
    "\n",
    "# To learn more about the solvers:\n",
    "# https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-defintions\n",
    "\n",
    "Spam_model.fit(X_train, Y_train)\n",
    "pred = Spam_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caveats:\n",
    "\n",
    "- this is not a cookbook that you can follow straight away and throw it on another dataset\n",
    "- data exploration and thinking of features is super important\n",
    "- parameter tuning, I made it seem really easy but there is a lot of work involved\n",
    "- regularization\n",
    "- cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thresholding ... Defining the classification boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our probabilities, what do we do with them? We have to find a threshold and classify based on that threshold. \n",
    "\n",
    "To understand what we need, let's do the following:\n",
    "1. Find all data points which had true value as \"spam\"\n",
    "2. Plot a histogram of the spam probabilities. \n",
    "\n",
    "Remember, if our classifier was perfect, it would have given high spam probability for all the data points which had true value as spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = Spam_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_spam = probs[:, 1]\n",
    "prob_ham = probs[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(prob_spam[Y_test == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that our classifier gets most of them right. Specifically, most of the data points that were indeed spam seem to have been assigned a high 'spam probability' value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is threshold? It is a value $\\eta$ such that whenever 'spam probability' is greater than $\\eta$, we will classify the data point as spam.\n",
    "\n",
    "So if we choose $\\eta$ to be 0, then we will definitely classify all data points which were indeed spam as spam. If we choose $\\eta$ to be 0.5 instead, we will misclassify all those data points which had 'spam probability' less than 0.5. The histogram helps get a visual on where we should really place our $\\eta$.\n",
    "\n",
    "Note, that there is a dual viewpoint here. Let's plot a histogram for ham as well. Here, the $\\eta$ works in an opposite way. Whatever we set $\\eta$ to, we will misclassify all the datapoints which had 'ham probability' larger than $\\eta$. That's why, let's place both histograms on top of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(prob_spam[Y_test == 0], color = 'orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now juxtapose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(prob_spam[Y_test == 1])\n",
    "plt.hist(prob_spam[Y_test == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, whatever we set our $\\eta$ to, on the left, we will misclassify those in blue, and on the right, we will misclassify those in orange. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(prob_ham[Y_test == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy: How many of your predictions matched the ground truth?\n",
    "print(metrics.accuracy_score(Y_test,pred))\n",
    "\n",
    "# Precision: Among the ones that you predicted as spam, how many were actually spam?\n",
    "print(metrics.precision_score(Y_test,pred))\n",
    "\n",
    "# Recall: Among the ones that were actually spam, how many did you predict as spam?\n",
    "print(metrics.recall_score(Y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy: \",get_accuracy(Y_test,pred))\n",
    "print(\"precision: \", get_precision(Y_test,pred))\n",
    "print(\"recall: \", get_recall(Y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.confusion_matrix(Y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With better features, better feature engineering and more powerful algorithms, we can always do better! The stakes can be very high in some classification problems (cancer or no cancer) and different evaluation measures say different things, one must always choose what would be best for the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data into a dataframe using pandas\n",
    "df = pd.read_csv('Spam_Ham_V2345.csv', sep = \",\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "training_data, testing_data = train_test_split(df,random_state = 2019, test_size = 0.2)\n",
    "training_data = training_data.head(212014) # to make sure we are working with the same data when we explore vowpal wabbit\n",
    "\n",
    "Y_train=training_data['label'].values\n",
    "Y_test=testing_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "BOW_transformer = CountVectorizer(stop_words='english')\n",
    "BOW_transformer.fit_transform(training_data['Message'].values)\n",
    "\n",
    "train_BOW_features = BOW_transformer.transform(training_data['Message'].values)\n",
    "test_BOW_features = BOW_transformer.transform(testing_data['Message'].values)\n",
    "\n",
    "# append length of message to features\n",
    "from scipy.sparse import hstack\n",
    "X_train = hstack((train_BOW_features,training_data['msg_len'].values[:,None]))\n",
    "X_test = hstack((test_BOW_features,testing_data['msg_len'].values[:,None]))\n",
    "\n",
    "Spam_model = LogisticRegression(solver='liblinear', penalty='l1') \n",
    "\n",
    "Spam_model.fit(X_train, Y_train)\n",
    "pred = Spam_model.predict(X_test)\n",
    "\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy: \",get_accuracy(Y_test,pred))\n",
    "print(\"precision: \", get_precision(Y_test,pred))\n",
    "print(\"recall: \", get_recall(Y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using Vowpal Wabbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vowpalwabbit import pyvw\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vw_format(document, label=None):\n",
    "      return str(label or '') + ' |text ' + ' '.join(re.findall('\\w{3,}', document.lower())) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.values[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_vw_format(str(training_data.values[1][2]), 1 if Y_train[0] == 1 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "vw_learn1 = pyvw.vw(\"--loss_function logistic -d ham_spam_train_v2345.vw -f vw_model\")\n",
    "vw_learn1.run_parser()\n",
    "vw_learn1.finish()\n",
    "\n",
    "vw_predict1 = pyvw.vw(\"-d ham_spam_test_v2345.vw -i vw_model -t -p vw_preds.txt --binary\")\n",
    "vw_predict1.run_parser()\n",
    "vw_predict1.finish()\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vw_preds.txt') as pred_file:\n",
    "    vw_preds = [float(label) \n",
    "                             for label in pred_file.readlines()]\n",
    "vw_preds_formatted = [0 if x == -1 else x for x in vw_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = []\n",
    "with open('ham_spam_test_v2345.vw', 'r') as f:\n",
    "    for line in f:\n",
    "        Y_test.append(float(line.split('|')[0]))\n",
    "f.close()\n",
    "Y_test_formatted = [0 if x == -1. else x for x in Y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy: \",get_accuracy(Y_test_formatted,vw_preds_formatted))\n",
    "print(\"precision: \", get_precision(Y_test_formatted,vw_preds_formatted))\n",
    "print(\"recall: \", get_recall(Y_test_formatted,vw_preds_formatted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about Vowpal Wabbit, visit http://vowpalwabbit.org/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to explore on your own:\n",
    "- [decision trees](https://scikit-learn.org/stable/modules/tree.html)\n",
    "- [random forests](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "- better feature engineering ([think tf-idf](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer))\n",
    "- regularization\n",
    "    - recap: in high dimensional data, you want to _regularize_ or shrink the coefficients of predictors to zero when you want a fit a model with all predictors in order to reduce variance\n",
    "     - [ridge regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n",
    "     - [lasso regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n",
    "- [cross validation](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "    - recap: in the absence of designated test set, how to estimate test error?\n",
    "- feature importance\n",
    "- multiclass classification\n",
    "- [naive bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "\n",
    "Beware of:\n",
    "- highly correlated variables\n",
    "- exploding features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, this tutorial is not meant to be a recipe that one can blindly follow. If it was, there would have never been a need for a Data Scientist :-) This tutorial was only to give a basic overview of linear regression and how to implement that using standard libraries. More importantly, I hope you now have an idea of what sort of questions one should ask while creating a mathematical model. We hope that this has piqued your interest to explore the capabilities of machine learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
